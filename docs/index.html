
<!DOCTYPE html>

<head>
<meta charset="utf-8" />
<title></title>
    
</head>
<body><style type="text/css">
	* {
		margin: 0;
		padding: 0;
	}
    ul li{
        list-style: none;
    }
    a {
        text-decoration: none;
        color:#333;
    }
    #menu {
        font:bold 16px "malgun gothic";
        width:580px;
        height:50px;
        background: #e0dddd;
        color:black;
        line-height: 50px; 
        margin:0 auto;
        text-align: center;
    }

    #menu > ul > li {
        float:left;
        width:140px;
        position:relative;
    }
    #menu > ul > li > ul {
    width:130px;
    display:none;
    position: absolute;
    font-size:14px;
    background: rgb(229, 243, 248);
    }
    #menu > ul > li:hover > ul {
        display:block;
    }
    </style>
<div id="menu">
	<ul>
		<li><a href="#">SEFD</a>
			<ul>
				<li><a href="#Abstract">Abstract</a></li>
				<li><a href="#method">Method</a></li>
				<li><a href="#adaptive">Adaptive Dilation</a></li>
			</ul>
		</li>
		<li><a href="#">Benchmark Result</a>
			<ul>
				<li><a href="#3DPW Benchmark and Occlusion & Complex Pose Dataset Results">Paper Result</a></li>
				<li><a href="#MuPoTs">MuPoTs</a></li>
			</ul>
		</li>
		<li><a href="#">Visualization</a>
			<ul>
				<li><a href="#Visualization Results">Main Paper</a></li>
				<li><a href="#Other Visualization Results">CrowdPose</a></li>
				<li><a href="#Other Visualization Results_3DPW">3DPW</a></li>
			</ul>
		</li>
		<li><a href="#">SEE & SESD</a>
			<ul>
				<li><a href="#SEE">SEE</a></li>
				<li><a href="#SESD">SESD</a></li>
			</ul>
		</li>
	</ul>
</div>
<br/>
    <div style="border: 2px solid rgb(209, 206, 206); border-radius: 15px; padding: 10px;width:1100px; margin:auto;box-shadow: 1px 1px 2px 6px rgb(209, 206, 206)">
        <br/>
    <h1><p style="font-size:1.3em;text-align:center; font-family: 'Times New Roman'">SEFD : Learning to Distill Complex Pose and Occlusion</p></h1>
    <br/>
    <h2><p style="color:gray;font-size:0.9em;text-align:center; font-family: 'Times New Roman'">Anonymous ICCV submission</p></h2>
    <br/>
    <h2><p style="color:gray;font-size:0.7em;text-align:center; font-family: 'Times New Roman';font-weight: normal;">Paper ID : 11451</p></h2>
    <br/>
    <div style="border: 2px solid white; border-radius: 15px; padding: 10px;width:1100px; margin:auto"></div>
    <center>
    <img src="smpl_edge_visualization_output.png" alt="My Image" width="1000">
    <h3><p style="font-size:0.9em;text-align:center; font-family: 'Times New Roman';font-weight: normal;">
        Qualitative comparison of the baseline 3DCrowdNet and the proposed feature distillation 
        SMPL overlapping edge. <br/>
        <strong>(a)</strong> and <strong>(b)</strong> show complex poses and <strong>(c)</strong> and <strong>(d)</strong> shows occluded situations.
        <br/>
        <br/>
    </p></h3>
    </center>
    </div>
    </div>
    <br/>
    <div style="border: 2px solid rgb(209, 206, 206); border-radius: 15px; padding: 10px;width:1100px; margin:auto;box-shadow: 1px 1px 2px 6px rgb(209, 206, 206)">
        <br/>
    <h2><p style="font-size:1.3em;text-align:center; font-family: 'Times New Roman';font-weight: normal;">Feature distillation Learning</p></h1>
        <br/>
    <div style="border: 2px solid white; border-radius: 15px; width:1000px; margin:auto;height: 200;">
    <h3><p style="color:black;font-size:0.9em; font-family: 'Times New Roman';font-weight: normal;">Conventional distillation methods for human poses aim to lighten student models. Our feature distillation aims to distill the ground-truth feature representations of the teacher model to the student model solving the real-environment condition without ground-truth.
        also, this is designed to reduce the structural gap between simple edge map and SMPL overlap edge</p></h3>

        <br/><p id="Abstract">
    </div>
    </div>
    <br/>
    <div style="border: 2px solid rgb(209, 206, 206); border-radius: 15px; padding: 10px;width:1100px; margin:auto;box-shadow: 1px 1px 2px 6px rgb(209, 206, 206)">
        <br/>
    <h2><p style="font-size:1.3em;text-align:center; font-family: 'Times New Roman';font-weight: normal;">Abstract</p></h2>
    <br/>
    <div style="border: 2px solid white; border-radius: 15px; width:1000px; margin:auto;height: 200;">
    <h3><p style="color:black;font-size:0.9em; font-family: 'Times New Roman';font-weight: normal;">This paper addresses the problem of three-dimensional (3D) human mesh estimation in complex poses and occluded situations. 
        Although many improvements have been made in 3D human mesh estimation using the two-dimensional (2D) pose with occlusion between humans, occlusion from complex poses and other objects remains a consistent problem. 
        Therefore, we propose the novel <strong>Skinned Multi-Person Linear (SMPL) Edge Feature Distillation (SEFD)</strong> that demonstrates robustness to complex poses and occlusions, without increasing the number of parameters compared to the baseline model. 
        The model generates an SMPL overlapping edge similar to the ground truth that contains target person boundary and occlusion information, performing subsequent feature distillation in a simple edge map. 
        We also perform experiments on various benchmarks and exhibit fidelity both qualitatively and quantitatively. 
        Extensive experiments prove that our method outperforms the state-of-the-art method by 2.8% in MPJPE and 1.9% in MPVPE on a benchmark 3DPW dataset in the presence of domain gap. 
        <strong>Also, our method is superior in 3DPW-OCC, 3DPW-PC, RH-Dataset, OCHuman, CrowdPose, and LSP dataset in which occlusion, com plex pose, and domain gap exist.</strong></p></h3>
        <br/><p id="method">
    </div>
    </div>
    <br/>
    <div style="border: 2px solid rgb(209, 206, 206); border-radius: 15px; padding: 10px;width:1100px; margin:auto;box-shadow: 1px 1px 2px 6px rgb(209, 206, 206)">
    <div style="border: 2px solid white; border-radius: 15px; width:1000px; margin:auto;height: 200;">
        <br/>
    <h2><p style="font-size:1.3em; font-family: 'Times New Roman';font-weight: normal;">SMPL Edge Feature Distillatino (SEFD)</p></h2>
    <br/>
    <h3><p style="color:black;font-size:0.9em; font-family: 'Times New Roman';font-weight: normal;">The figure below is the overall flow of our method.
        It consists of four components: Input Stage, SMPL Edge Generator, Teacher Model Training, and Student Model Training.
        To train the Teacher Model, an SMPL edge map must be generated through the SMPL Edge Generator in the Input Stage. After this process, the generated SMPL edge map is concatenated with the Input image and used to train the Teacher Model.
        After training the Teacher Model in this way, only the encoder of the Teacher Model is used to train the encoder of the Student Model through feature distillation. The input to the Student Model is obtained by passing it through a simple edge detector (e.g. Canny edge).</p></h3>
    
        <br/>
        <img src="main_model.png" alt="My Image" width="1000">
        <br/>
    <h3><p style="color:black;font-size:0.9em; font-family: 'Times New Roman';font-weight: normal;">To elaborate further, the SMPL Edge Generator consists of Projection, Edge Detection, Adaptive Dilation, and Overlap. For the Loss in Feature Distillation, 
        we used the Log Softmax Loss that we found, and we connected the 3rd and 4th feature maps for Feature Connection. 
        An explanation of Adaptive Dilation is provided below, and the reason for selecting the Feature Connection number and Log Softmax Loss and the results are mentioned below. <br/>
        <strong>We have added a GIF animation below just in case you have trouble understanding. Please check it out if you need further clarification.</strong></p></h3>
    
        <img src="main_model_gif.gif" alt="My Image" width="1000">
    
    <br/><p id="Visualization Results"></p>
    </div>
    </div>
    <br/>
    <div style="border: 2px solid rgb(209, 206, 206); border-radius: 15px; padding: 10px;width:1100px; margin:auto;box-shadow: 1px 1px 2px 6px rgb(209, 206, 206)">
        <br/>
        <div style="border: 2px solid white; border-radius: 15px; width:1000px; margin:auto;height: 200;">
    <h2><p style="font-size:1.3em; font-family: 'Times New Roman';font-weight: normal;">Visualization Results</p></h2>
    <br/>
    <img src="SOTA.png" alt="My Image" width="1000">
    
    <br/>
    <h3><p style="color:black;font-size:0.9em; font-family: 'Times New Roman';font-weight: normal;">The above result image is a comparison of our method with other State-of-the-Art methods such as I2L-MeshNet, SPIN, and 3DCrowdNet. 
        Only the 3DCrowdNet, which considers occlusion, and our method produced plausible results. In the first row, although 3DCrowdNet plausibly detects the mesh, it fails to properly extract the person at the very back. 
        On the other hand, our method successfully extracts all three individuals plausibly. In the second row, when the person at the front performs a complex pose, 3DCrowdNet misses it, whereas our method detects it plausibly. 
        In the last row, due to the complex pose, 3DCrowdNet fails to resolve the issue of the hand being projected forward, but our method shows a visually appealing result with all hands positioned behind the individuals.
        
    <br/>
        <br/><p  id="3DPW Benchmark and Occlusion & Complex Pose Dataset Results"></p>
    </div>
    </div>
    <br/>
    <div style="border: 2px solid rgb(209, 206, 206); border-radius: 15px; padding: 10px;width:1100px; margin:auto;box-shadow: 1px 1px 2px 6px rgb(209, 206, 206)">
    
        <br/>
        <div style="border: 2px solid white; border-radius: 15px; width:1000px; margin:auto;height: 200;">
    <h2><p style="font-size:1.3em; font-family: 'Times New Roman';font-weight: normal;">3DPW Benchmark and Occlusion & Complex Pose Dataset Results</p></h2>
    <br/>
    <center><img src="test_table4.png" alt="My Image" width="700"></center>
    <br/>
    <h3><p style="color:black;font-size:0.9em; font-family: 'Times New Roman';font-weight: normal;">This table shows Table 4 from the main paper, which categorizes methods into those that use the training dataset for the 3DPW Benchmark and those that do not. 
        The reason for categorizing models that do not use the training dataset is to demonstrate how well they perform in the presence of domain gaps. Our model shows superior results in this table. 
        Furthermore, a comparison of our model with the current SOTA model, CLIFF, is shown in the table below, which compares their robustness in the presence of occlusion and complex poses when a domain gap exists. 
        
<br/>
    <br/>
        <center><img src="test_table5.png" alt="My Image" width="1000"></center>
        <br/>
        <h3><p style="color:black;font-size:0.9em; font-family: 'Times New Roman';font-weight: normal;">The table above is Table 5 in the main paper, which consists of datasets for occlusion and complex poses. 
            The occlusion dataset includes 3DPW-OCC, 3DPW-PC, RH-Dataset (RH-D), OCHuman, and CrowdPose, while the complex pose dataset is LSP. The methods compared are all targeted towards occlusion, including OCHMR, Liu et al, VisDB, and 3DCrowdNet. 
            CLIFF was conducted to assess the difference in accuracy between our method and theirs in the presence of a domain gap. Our method shows overwhelmingly better performance compared to other methods. 
            <br/><p id="MuPoTs"></p>
            <br/>
        </div>
    </div>
    <br/>
    <div style="border: 2px solid rgb(209, 206, 206); border-radius: 15px; padding: 10px;width:1100px; margin:auto;box-shadow: 1px 1px 2px 6px rgb(209, 206, 206)">
    
        <br/>
        <div style="border: 2px solid white; border-radius: 15px; width:1000px; margin:auto;height: 200;">
    <h2><p style="font-size:1.3em; font-family: 'Times New Roman';font-weight: normal;">MuPoTs Dataset Results</p></h2>
    <br/>
    <center><img src="MuPoTs.png" alt="My Image" width="400"></center>
    <br/>
    <h3><p style="color:black;font-size:0.9em; font-family: 'Times New Roman';font-weight: normal;">The table shows the results presented in the supplementary materials. 
        While we used MuCo as the training dataset, the MuPoTs test set, which is a test set of MuCo, was shot externally rather than internally to create the largest possible domain gap with the training dataset. 
        Therefore, the results on MuPoTs indicate the presence of a domain gap, and demonstrate that our method performs superiorly under such conditions.
    <br/><p id="adaptive"></p>
    <br/>
    </div>
    </div>
    
    <br/>
    <div style="border: 2px solid rgb(209, 206, 206); border-radius: 15px; padding: 10px;width:1100px; margin:auto;box-shadow: 1px 1px 2px 6px rgb(209, 206, 206)">

        <br/>
        <div style="border: 2px solid white; border-radius: 15px; width:1000px; margin:auto;height: 200;">
    <h2><p style="font-size:1.3em; font-family: 'Times New Roman';font-weight: normal;">Adaptive Dilation</p></h2>
    <br/>
    <center><img src="small_object.png" alt="My Image" width="1000"></center>
    <br/>
    <h3><p style="color:black;font-size:0.9em; font-family: 'Times New Roman';font-weight: normal;">The image shows, from left to right, the input image, the mesh result for the pseudo ground truth in MSCOCO, the result of Canny Edge, the result of Canny Edge with dilation kernel 5, and the result of Canny Edge with dilation kernel 9. 
        Starting from the result (c), we can observe that without dilation at a very small scale, we can infer the human form and pose. 
        However, from a kernel size of 5 or more, we can observe that the structural information of the human is distorted. 
        Therefore, we realized the need to adaptively adjust dilation from small to large scales, and solved this problem by adjusting the dilation kernel according to the area of the bounding box, as shown in the table below.
        <br/>
        <br/>
        <center><img src="adaptive_dilation_table.png" alt="My Image" width="500"></center>
        <br/>
        <h3><p style="color:black;font-size:0.9em; font-family: 'Times New Roman';font-weight: normal;">
            The table above shows the dilation kernel size for different bounding box areas. 
            We used a histogram to determine the criteria for adaptive dilation, and the results are shown in the figure below. 
            The blue color represents the result obtained when the dilation kernel size is fixed to 5, and the red color represents the result obtained when adaptive dilation is applied.
            Compared to the blue edge map, we can see that adaptive dilation better captures structural information for small-scale objects.
            <br/>
        <br/>
        <center><img src="apdaptive_dilation.png" alt="My Image" width="500"></center>
        
        <center><img src="adaptive_dilationsupple.png" alt="My Image" width="500"></center>

        <br/><p id="SEE"></p>
        </div>
    </div>
    <br/>
    <div style="border: 2px solid rgb(209, 206, 206); border-radius: 15px; padding: 10px;width:1100px; margin:auto;box-shadow: 1px 1px 2px 6px rgb(209, 206, 206)">
    
        <br/>
        <div style="border: 2px solid white; border-radius: 15px; width:1000px; margin:auto;height: 200;">
    <h2><p style="font-size:1.3em; font-family: 'Times New Roman';font-weight: normal;">SMPL Edge Estimator (SEE)</p></h2>
    <br/>
    <center><img src="SEE_backbone.png" alt="My Image" width="520"><img src="SEE_result.png" alt="My Image" width="480"></center>
    <br/>
    <h3><p style="color:black;font-size:0.9em; font-family: 'Times New Roman';font-weight: normal;">The above image shows the visualization of the "SMPL Edge Estimator" module that directly estimates the SMPL Edge map without feature distillation. 
        The motivation for conducting this experiment was to see if it is possible to create a module that can successfully generate the SMPL Edge map using a different backbone than feature distillation. 
        For the implementation details of the simple SEE, a UNet model commonly used in segmentation was used, and the experiment was conducted using the H36M, MuCo, AGORA, MSCOCO pseudo GT, and MPII pseudo GT datasets. 
        The loss function used was L1 Loss and VGG 16 Loss, and SSIM Loss, and the best results were obtained when using L1 * 0.4 + VGG 16 feature map * 1.6. The experiment was conducted for 80 epochs, and an RTX 3090 GPU was used. 
        While the SEE experiment we conducted yielded reasonably good results, there was a critical issue as shown in the image below.
        <br/>
        <br/>
        <center><img src="SEE_complex_Occlusion.png" alt="My Image" width="1000"></center>
        <br/>
        <h3><p style="color:black;font-size:0.9em; font-family: 'Times New Roman';font-weight: normal;">The above figure shows the results of testing SEE's robustness in complex poses and occlusion situations. 
            As shown in the results for easy-to-predict poses, it can be seen that the UNet backbone estimates the edges reasonably well, 
            but in cases of occlusion, it fails to detect any edge, which is different from our expectations of detecting rough edges in occlusion situations. 
            Furthermore, it can be observed that complex poses such as sitting are not properly detected. 
            Therefore, we designed a more secure model and created the SMPL Edge Self-supervised De-occlusion (SESD) module. We will explain SESD in detail in the next section.
    <br/><p id="SESD"></p>
    <br/>
    </div>
    </div>
    <br/>
    <div style="border: 2px solid rgb(209, 206, 206); border-radius: 15px; padding: 10px;width:1100px; margin:auto;box-shadow: 1px 1px 2px 6px rgb(209, 206, 206)">
    
        <br/>
        <div style="border: 2px solid white; border-radius: 15px; width:1000px; margin:auto;height: 200;">
    <h2><p style="font-size:1.3em; font-family: 'Times New Roman';font-weight: normal;">SMPL Edge Self-supervised De-occlusion (SESD)</p></h2>
    <br/>
    <center><img src="SESD_backbone.png" alt="My Image" width="1000"></center>
    <br/>
    <h3><p style="color:black;font-size:0.9em; font-family: 'Times New Roman';font-weight: normal;">The above figure shows the SESD module, which was developed to address the limitations of the SEE model in detecting edges in complex poses and occlusion situations. 
        The implementation details of SESD were similar to those of SEE, where UNet was used as the backbone for estimating the SMPL Edge map and Segmentation map from the input image. 
        In the training stage, the SMPL Edge map and Segmentation map were estimated, and in the second stage, the estimated Segmentation map was multiplied with a random segmentation to erase the corresponding regions.
         The resulting output segmentation was then multiplied with the input image, and this was fed into the first stage and a different UNet backbone to estimate the SMPL Edge map. 
         The self-supervised learning approach was used to train the model, where the first stage's edge map was re-estimated using the obtained SMPL Edge map. 
         During the testing stage, only the segmentation map was used from the first stage, and the input image was multiplied with it before being fed into the second stage's UNet model to estimate the SMPL Edge map. 
         This idea was inspired by Zhan et al.'s paper, "Self-Supervised Scene De-occlusion." For any readers who may still be confused, I have created the following animated GIF to help illustrate.
        <br/>
        <center><img src="SESD_expalin.gif" alt="My Image" width="1000"></center>
        <br/>
    <br/><p id="Other Visualization Results"></p>
    <br/>
    </div>
    </div>
    <br/>
    <div style="border: 2px solid rgb(209, 206, 206); border-radius: 15px; padding: 10px;width:1100px; margin:auto;box-shadow: 1px 1px 2px 6px rgb(209, 206, 206)">
        <br/>
    <div style="border: 2px solid white; border-radius: 15px; width:1000px; margin:auto;height: 200;">
    <h2><p style="font-size:1.3em; font-family: 'Times New Roman';font-weight: normal;">Other Visualization Results</p></h2>
    <br/>
    <img src="complex_case.png" alt="My Image" width="1000">
    <img src="Occlusion_case.png" alt="My Image" width="1000">
    <img src="Occlusion+Complex_case.png" alt="My Image" width="1000">
    <br/>
    <br/>
    <h3><p style="color:black;font-size:0.9em; font-family: 'Times New Roman';font-weight: normal;">The above images show, from left to right, the input image, input 2D pose, 3DCrowdNet, and our method, respectively. 
        All of these images are from the CrowdPose test set and demonstrate how our method is more robust in complex poses and occlusion compared to 3DCrowdNet. 
        They demonstrate how well our method performs in challenging situations involving complex poses and occlusion.

        <br/><p id="Other Visualization Results_3DPW"></p>
        <br/>
    </div>
    </div>
    <br/>
    <div style="border: 2px solid rgb(209, 206, 206); border-radius: 15px; padding: 10px;width:1100px; margin:auto;box-shadow: 1px 1px 2px 6px rgb(209, 206, 206)">
        <br/>
    <div style="border: 2px solid white; border-radius: 15px; width:1000px; margin:auto;height: 200;">
    <h2><p style="font-size:1.3em; font-family: 'Times New Roman';font-weight: normal;">Other Visualization Results</p></h2>
    <br/>
    <img src="supple_main5.png" alt="My Image" width="1000">
    <br/>
    <br/>
    <h3><p style="color:black;font-size:0.9em; font-family: 'Times New Roman';font-weight: normal;">The image results compare the 3D human modeling results on the 3DPW test set. 
        The first row shows the input image, 2D pose, and SMPL model edges. The second row compares the GT Mesh (ground truth), Baseline Mesh, and SEFD's Mesh results. 
        The third row shows a detailed comparison of the differences between the meshes.
        These image results demonstrate that our method performs well on the 3DPW test set, and works better than 3DCrowdNet in more complex poses and occlusions. 
        Therefore, these results show that our method performs well in the field of 3D human modeling and works well in various challenging situations.

        <br/>
        <br/>
    </div>
    </div>
    <div style="cursor:pointer;position: fixed;bottom: 1vw;right: 1vw;background-color: gray;opacity: 50%;border-radius: 10px; width:4vw;text-align: center;" onclick="window.scrollTo(0,0);"><p style="font-size: 1.5vw">TOP</p></div>
    </div>
</body>
</html>
