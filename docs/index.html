<!DOCTYPE html>
<html>
<head>
</head>
<body>
    <div style="border: 2px solid rgb(209, 206, 206); border-radius: 15px; padding: 10px;width:1100px; margin:auto;box-shadow: 1px 1px 2px 6px rgb(209, 206, 206)">
    <h1><p style="font-size:1.3em;text-align:center; font-family: 'Times New Roman'">SEFD : Learning to Distill Complex Pose and Occlusion</p></h1>
    <h2><p style="color:gray;font-size:0.9em;text-align:center; font-family: 'Times New Roman'">Anonymous ICCV submission</p></h2>
    <h2><p style="color:gray;font-size:0.7em;text-align:center; font-family: 'Times New Roman';font-weight: normal;">Paper ID : 11451</p></h2>
    <div style="border: 2px solid white; border-radius: 15px; padding: 10px;width:1100px; margin:auto"></div>
    <center>
    <img src="smpl_edge_visualization_output.png" alt="My Image" width="1000">
    <h3><p style="font-size:0.9em;text-align:center; font-family: 'Times New Roman';font-weight: normal;">
        Qualitative comparison of the baseline 3DCrowdNet and the proposed feature distillation 
        SMPL overlapping edge. <br/>
        <strong>(a)</strong> and <strong>(b)</strong> show complex poses and <strong>(c)</strong> and <strong>(d)</strong> shows occluded situations.
    </p></h3>
    </center>
    </div>
    </div>
    <br/>
    <div style="border: 2px solid rgb(209, 206, 206); border-radius: 15px; padding: 10px;width:1100px; margin:auto;box-shadow: 1px 1px 2px 6px rgb(209, 206, 206)">
    <h2><p style="font-size:1.3em;text-align:center; font-family: 'Times New Roman';font-weight: normal;">Appendix</p></h1>
    <div style="border: 2px solid white; border-radius: 15px; width:1000px; margin:auto;height: 200;">
    <h3><p style="color:black;font-size:0.9em; font-family: 'Times New Roman';font-weight: normal;">For convenience, we have added internal links so that you can directly navigate to the desired location or result. 
        If you want to go to the desired location, please click on the link below to move.</p></h3>
        <h3><a style="color:black;font-size:0.9em; font-family: 'Times New Roman';font-weight: normal;" href="#Abstract">1. Abstract</a></h3>
        <h3><a style="color:black;font-size:0.9em; font-family: 'Times New Roman';font-weight: normal;" href="#method">2. Method</a></h3>
        <h3><a style="color:black;font-size:0.9em; font-family: 'Times New Roman';font-weight: normal;" href="#Visualization Results">3. Visualization result</a></h3>
        <h3><a style="color:black;font-size:0.9em; font-family: 'Times New Roman';font-weight: normal;" href="#3DPW Benchmark and Occlusion & Complex Pose Dataset Results">4. 3DPW Benchmark and Occlusion & Complex Pose Dataset Results</a></h3>
        <h3><a style="color:black;font-size:0.9em; font-family: 'Times New Roman';font-weight: normal;" href="#Other Visualization Results">5. Other Visualization result (CrowdPose)</a></h3>
    </div>
    </div>
    <br/>
    <div style="border: 2px solid rgb(209, 206, 206); border-radius: 15px; padding: 10px;width:1100px; margin:auto;box-shadow: 1px 1px 2px 6px rgb(209, 206, 206)">
    <h2><p style="font-size:1.3em;text-align:center; font-family: 'Times New Roman';font-weight: normal;">Feature distillation Learning</p></h1>
    <div style="border: 2px solid white; border-radius: 15px; width:1000px; margin:auto;height: 200;">
    <h3><p style="color:black;font-size:0.9em; font-family: 'Times New Roman';font-weight: normal;">Conventional distillation methods for human poses aim to lighten student models. Our feature distillation aims to distill the ground-truth feature representations of the teacher model to the student model solving the real-environment condition without ground-truth.
        also, this is designed to reduce the structural gap between simple edge map and SMPL overlap edge</p></h3>
    </div>
    </div>
    <br/>
    <div style="border: 2px solid rgb(209, 206, 206); border-radius: 15px; padding: 10px;width:1100px; margin:auto;box-shadow: 1px 1px 2px 6px rgb(209, 206, 206)">
    <h2><p style="font-size:1.3em;text-align:center; font-family: 'Times New Roman';font-weight: normal;" id="Abstract">Abstract</p></h1>
    <div style="border: 2px solid white; border-radius: 15px; width:1000px; margin:auto;height: 200;">
    <h3><p style="color:black;font-size:0.9em; font-family: 'Times New Roman';font-weight: normal;">This paper addresses the problem of three-dimensional (3D) human mesh estimation in complex poses and occluded situations. 
        Although many improvements have been made in 3D human mesh estimation using the two-dimensional (2D) pose with occlusion between humans, occlusion from complex poses and other objects remains a consistent problem. 
        Therefore, we propose the novel <strong>Skinned Multi-Person Linear (SMPL) Edge Feature Distillation (SEFD)</strong> that demonstrates robustness to complex poses and occlusions, without increasing the number of parameters compared to the baseline model. 
        The model generates an SMPL overlapping edge similar to the ground truth that contains target person boundary and occlusion information, performing subsequent feature distillation in a simple edge map. 
        We also perform experiments on various benchmarks and exhibit fidelity both qualitatively and quantitatively. 
        Extensive experiments prove that our method outperforms the state-of-the-art method by 2.8% in MPJPE and 1.9% in MPVPE on a benchmark 3DPW dataset in the presence of domain gap. 
        <strong>Also, our method is superior in 3DPW-OCC, 3DPW-PC, RH-Dataset, OCHuman, CrowdPose, and LSP dataset in which occlusion, com plex pose, and domain gap exist.</strong></p></h3>
    </div>
    </div>
    <br/>
    <div style="border: 2px solid rgb(209, 206, 206); border-radius: 15px; padding: 10px;width:1100px; margin:auto;box-shadow: 1px 1px 2px 6px rgb(209, 206, 206)">
    <div style="border: 2px solid white; border-radius: 15px; width:1000px; margin:auto;height: 200;">
    <h2><p style="font-size:1.3em; font-family: 'Times New Roman';font-weight: normal;" id="method">SMPL Edge Feature Distillatino (SEFD)</p></h2>
    <h3><p style="color:black;font-size:0.9em; font-family: 'Times New Roman';font-weight: normal;">The figure below is the overall flow of our method.
        It consists of four components: Input Stage, SMPL Edge Generator, Teacher Model Training, and Student Model Training.
        To train the Teacher Model, an SMPL edge map must be generated through the SMPL Edge Generator in the Input Stage. After this process, the generated SMPL edge map is concatenated with the Input image and used to train the Teacher Model.
        After training the Teacher Model in this way, only the encoder of the Teacher Model is used to train the encoder of the Student Model through feature distillation. The input to the Student Model is obtained by passing it through a simple edge detector (e.g. Canny edge).</p></h3>
    <img src="main_model.png" alt="My Image" width="1000">
    <h3><p style="color:black;font-size:0.9em; font-family: 'Times New Roman';font-weight: normal;">To elaborate further, the SMPL Edge Generator consists of Projection, Edge Detection, Adaptive Dilation, and Overlap. For the Loss in Feature Distillation, 
        we used the Log Softmax Loss that we found, and we connected the 3rd and 4th feature maps for Feature Connection. 
        An explanation of Adaptive Dilation is provided below, and the reason for selecting the Feature Connection number and Log Softmax Loss and the results are mentioned below. <br/>
        <strong>We have added a GIF animation below just in case you have trouble understanding. Please check it out if you need further clarification.</strong></p></h3>
    <img src="main_model_gif.gif" alt="My Image" width="1000">
    </div>
    </div>
    <br/>
    <div style="border: 2px solid rgb(209, 206, 206); border-radius: 15px; padding: 10px;width:1100px; margin:auto;box-shadow: 1px 1px 2px 6px rgb(209, 206, 206)">
    <div style="border: 2px solid white; border-radius: 15px; width:1000px; margin:auto;height: 200;">
    <h2><p style="font-size:1.3em; font-family: 'Times New Roman';font-weight: normal;" id="Visualization Results">Visualization Results</p></h2>
    <img src="SOTA.png" alt="My Image" width="1000">
    <h3><p style="color:black;font-size:0.9em; font-family: 'Times New Roman';font-weight: normal;">The above result image is a comparison of our method with other State-of-the-Art methods such as I2L-MeshNet, SPIN, and 3DCrowdNet. 
        Only the 3DCrowdNet, which considers occlusion, and our method produced plausible results. In the first row, although 3DCrowdNet plausibly detects the mesh, it fails to properly extract the person at the very back. 
        On the other hand, our method successfully extracts all three individuals plausibly. In the second row, when the person at the front performs a complex pose, 3DCrowdNet misses it, whereas our method detects it plausibly. 
        In the last row, due to the complex pose, 3DCrowdNet fails to resolve the issue of the hand being projected forward, but our method shows a visually appealing result with all hands positioned behind the individuals.
    </div>
    </div>
    <br/>
    <div style="border: 2px solid rgb(209, 206, 206); border-radius: 15px; padding: 10px;width:1100px; margin:auto;box-shadow: 1px 1px 2px 6px rgb(209, 206, 206)">
    <div style="border: 2px solid white; border-radius: 15px; width:1000px; margin:auto;height: 200;">
    <h2><p style="font-size:1.3em; font-family: 'Times New Roman';font-weight: normal;" id="3DPW Benchmark and Occlusion & Complex Pose Dataset Results">3DPW Benchmark and Occlusion & Complex Pose Dataset Results</p></h2>
    <center><img src="test_table4.png" alt="My Image" width="700"></center>
    <h3><p style="color:black;font-size:0.9em; font-family: 'Times New Roman';font-weight: normal;">This table shows Table 4 from the main paper, which categorizes methods into those that use the training dataset for the 3DPW Benchmark and those that do not. 
        The reason for categorizing models that do not use the training dataset is to demonstrate how well they perform in the presence of domain gaps. Our model shows superior results in this table. 
        Furthermore, a comparison of our model with the current SOTA model, CLIFF, is shown in the table below, which compares their robustness in the presence of occlusion and complex poses when a domain gap exists. 
        <center><img src="test_table5.png" alt="My Image" width="1000"></center>
        <h3><p style="color:black;font-size:0.9em; font-family: 'Times New Roman';font-weight: normal;">The table above is Table 5 in the main paper, which consists of datasets for occlusion and complex poses. 
            The occlusion dataset includes 3DPW-OCC, 3DPW-PC, RH-Dataset (RH-D), OCHuman, and CrowdPose, while the complex pose dataset is LSP. The methods compared are all targeted towards occlusion, including OCHMR, Liu et al, VisDB, and 3DCrowdNet. 
            CLIFF was conducted to assess the difference in accuracy between our method and theirs in the presence of a domain gap. Our method shows overwhelmingly better performance compared to other methods. 
    </div>
    </div>
    <br/>
    <div style="border: 2px solid rgb(209, 206, 206); border-radius: 15px; padding: 10px;width:1100px; margin:auto;box-shadow: 1px 1px 2px 6px rgb(209, 206, 206)">
    <div style="border: 2px solid white; border-radius: 15px; width:1000px; margin:auto;height: 200;">
    <h2><p style="font-size:1.3em; font-family: 'Times New Roman';font-weight: normal;"id="Other Visualization Results">Other Visualization Results</p></h2>
    <img src="complex_case.png" alt="My Image" width="1000">
    <img src="Occlusion_case.png" alt="My Image" width="1000">
    <img src="Occlusion+Complex_case.png" alt="My Image" width="1000">
    <h3><p style="color:black;font-size:0.9em; font-family: 'Times New Roman';font-weight: normal;">The above images show, from left to right, the input image, input 2D pose, 3DCrowdNet, and our method, respectively. 
        All of these images are from the CrowdPose test set and demonstrate how our method is more robust in complex poses and occlusion compared to 3DCrowdNet. 
        They demonstrate how well our method performs in challenging situations involving complex poses and occlusion.

        <br/>
        <br/>
    </div>
    </div>

</body>
</html>
